# -*- coding: utf-8 -*-
"""assignment_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H6QUi32to3hj1uO9vXiINgjfQG2iLQri

Q1. Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset.
"""

import pandas as pd
import numpy as np
from keras.datasets import fashion_mnist
import wandb
from sklearn.metrics import confusion_matrix


wandb.init(project="DLA1", entity="da24m014-iit-madras")

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()



class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

sample_images = []
unique_classes = np.unique(y_train)

for cls in unique_classes:
    sample_idx = np.where(y_train == cls)[0][0]  
    img = x_train[sample_idx]

    sample_images.append(wandb.Image(img, caption=class_names[cls]))

wandb.log({"Sample Images": sample_images})

wandb.finish()

# (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train, x_test = x_train.reshape(x_train.shape[0], -1), x_test.reshape(x_test.shape[0], -1)
num_classes = 10

def one_hot_encode(y, num_classes):
    encoded = np.zeros((y.size,num_classes))
    encoded[np.arange(y.size),y] = 1
    return encoded

y_train, y_test = one_hot_encode(y_train,num_classes), one_hot_encode(y_test, num_classes)

split_idx = int(0.9*len(x_train))
x_train, x_val = x_train[:split_idx], x_train[split_idx:]
y_train, y_val = y_train[:split_idx], y_train[split_idx:]

# Activation Functions
def relu(Z):
    return np.maximum(0, Z)

def sigmoid(Z):
    Z = np.clip(Z, -500, 500)
    return 1 / (1 + np.exp(-Z))

def tanh(Z):
    return np.tanh(Z)

def softmax(Z):
    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))
    return expZ / np.sum(expZ, axis=1, keepdims=True)

activation_functions = {"relu": relu, "sigmoid": sigmoid, "tanh": tanh}

def relu_derivative(A):
    return (A > 0).astype(float)

def sigmoid_derivative(A):
    return A * (1 - A)  

def tanh_derivative(A):
    return 1 - A**2  

activation_derivatives = {
    "relu": relu_derivative,
    "sigmoid": sigmoid_derivative,
    "tanh": tanh_derivative
}

import numpy as np

# Stochastic Gradient Descent (SGD)
def sgd_update(weights, biases, grads_W, grads_b, learning_rate):
    for i in range(len(weights)):
        weights[i] -= learning_rate * grads_W[i]
        biases[i] -= learning_rate * grads_b[i]

# Momentum Optimizer
def momentum_update(weights, biases, grads_W, grads_b, learning_rate, velocity_W, velocity_b, momentum=0.5):
    for i in range(len(weights)):
        velocity_W[i] = momentum * velocity_W[i] - learning_rate * grads_W[i]
        velocity_b[i] = momentum * velocity_b[i] - learning_rate * grads_b[i]  

        weights[i] += velocity_W[i]
        biases[i] += velocity_b[i] 

# Nesterov Accelerated Gradient (NAG)
def nesterov_update(weights, biases, grads_W, grads_b, learning_rate, velocity_W,velocity_b, momentum=0.5):
    for i in range(len(weights)):
        lookahead_W = weights[i] + momentum * velocity_W[i]
        lookahead_b = biases[i] + momentum * velocity_b[i] 

        velocity_W[i] = momentum * velocity_W[i] - learning_rate * grads_W[i]
        velocity_b[i] = momentum * velocity_b[i] - learning_rate * grads_b[i]  

        weights[i] = lookahead_W + velocity_W[i]
        biases[i] = lookahead_b + velocity_b[i]

# RMSprop Optimizer
def rmsprop_update(weights, biases, grads_W, grads_b, learning_rate, velocity_W, velocity_b, beta=0.5, epsilon=1e-6):
    for i in range(len(weights)):
        velocity_W[i] = beta * velocity_W[i] + (1 - beta) * (grads_W[i] ** 2)
        velocity_b[i] = beta * velocity_b[i] + (1 - beta) * (grads_b[i] ** 2)

        weights[i] -= learning_rate * grads_W[i] / (np.sqrt(velocity_W[i]) + epsilon)

        biases[i] -= learning_rate * grads_b[i] / (np.sqrt(velocity_b[i]) + epsilon)

# Adam Optimizer
def adam_update(weights, biases, grads_W, grads_b, learning_rate, velocity_W, velocity_b, moment2_W, moment2_b, beta1=0.5, beta2=0.5, epsilon=1e-6, t=1):
    for i in range(len(weights)):
        velocity_W[i] = beta1 * velocity_W[i] + (1 - beta1) * grads_W[i]
        velocity_b[i] = beta1 * velocity_b[i] + (1 - beta1) * grads_b[i]

        moment2_W[i] = beta2 * moment2_W[i] + (1 - beta2) * (grads_W[i] ** 2)
        moment2_b[i] = beta2 * moment2_b[i] + (1 - beta2) * (grads_b[i] ** 2)

        velocity_W_corrected = velocity_W[i] / (1 - beta1 ** t)
        velocity_b_corrected = velocity_b[i] / (1 - beta1 ** t)

        moment2_W_corrected = moment2_W[i] / (1 - beta2 ** t)
        moment2_b_corrected = moment2_b[i] / (1 - beta2 ** t)

        if moment2_b_corrected.shape != biases[i].shape:
            print(f"Shape mismatch at layer {i}: {moment2_b_corrected.shape} vs {biases[i].shape}")
            moment2_b_corrected = np.reshape(moment2_b_corrected, biases[i].shape)

        weights[i] -= learning_rate * velocity_W_corrected / (np.sqrt(moment2_W_corrected) + epsilon)
        biases[i] -= learning_rate * velocity_b_corrected / (np.sqrt(moment2_b_corrected) + epsilon)

    return t + 1  


class NeuralNetwork:
    def __init__(self, layers, learning_rate=0.1, activation="sigmoid", optimizer="sgd",
                 weight_init="random", weight_decay=0.0, beta=0.5, beta1=0.5, beta2=0.5, epsilon=1e-6, loss_function="cross_entropy"):
        self.layers = layers
        self.activation = activation
        self.optimizer = optimizer
        self.learning_rate = learning_rate
        self.weight_init = weight_init
        self.weight_decay = weight_decay
        self.beta = beta
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.loss_function = loss_function

        self.init_weights(weight_init)

        self.velocity_W = [np.zeros_like(W) for W in self.weights]
        self.velocity_b = [np.zeros_like(b) for b in self.biases]
        self.moment2_W = [np.zeros_like(W) for W in self.weights]
        self.moment2_b = [np.zeros_like(b) for b in self.biases]
        self.t = 0  

        global best_model_data
        best_model_data = {
            "weights": None,
            "biases": None,
            "params": None,
            "best_accuracy": 0
        }

    def init_weights(self, method):
        self.weights = []
        self.biases = []
        for i in range(len(self.layers) - 1):
            if method == "xavier":
                limit = np.sqrt(6 / (self.layers[i] + self.layers[i+1]))
            else:
                limit = 0.1
            W = np.random.uniform(-limit, limit, (self.layers[i], self.layers[i+1]))
            self.weights.append(W)
            self.biases.append(np.zeros((1, self.layers[i+1])))

    def forward(self, X):
        self.A = [X]
        for i in range(len(self.weights) - 1):
            Z = self.A[-1] @ self.weights[i] + self.biases[i]
            A = activation_functions[self.activation](Z)
            self.A.append(A)
        Z = self.A[-1] @ self.weights[-1] + self.biases[-1]
        A = softmax(Z)
        self.A.append(A)
        return A
    
    def compute_cross_entropy_loss(self, y_true,y_pred):
        loss = -np.mean(np.sum(y_true*np.log(y_pred + 1e-8),axis=1))
        return loss

    def compute_mse_loss(self,y_true,y_pred):
        loss = np.mean(np.sum((y_true - y_pred)**2,axis=1))
        return loss
    def compute_loss(self, y_true, y_pred):
        if self.loss_function == "cross_entropy":
            loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))
        else:
            loss = np.mean(np.sum((y_true - y_pred)**2,axis =1))

        loss+= (self.weight_decay / 2) * sum(np.sum(W**2) for W in self.weights)
        return loss

    def backward(self, X, y):
        grads_W, grads_b = [], []
        if self.loss_function == "cross_entropy":
            dA = self.A[-1]-y
        else:
            dA = 2*(self.A[-1]-y)
        for i in reversed(range(len(self.weights))):
            dW = self.A[i].T @ dA / X.shape[0]
            db = np.sum(dA, axis=0, keepdims=True) / X.shape[0]
            dW += self.weight_decay * self.weights[i]
            grads_W.append(dW)
            grads_b.append(db)
            if i > 0:
                dA = (dA @ self.weights[i].T) * activation_derivatives[self.activation](self.A[i])
        return grads_W[::-1], grads_b[::-1]

    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size):
        
        
        num_samples = X_train.shape[0]

        for epoch in range(epochs):
            indices = np.random.permutation(num_samples)
            X_train, y_train = X_train[indices], y_train[indices]

            total_loss, total_acc = 0, 0
            total_ce_loss,total_mse_loss=0, 0
            num_batches = num_samples // batch_size

            for i in range(0, num_samples, batch_size):
                X_batch = X_train[i:i + batch_size]
                y_batch = y_train[i:i + batch_size]

                y_pred = self.forward(X_batch)

                loss = self.compute_loss(y_batch, y_pred)
                ce_loss = self.compute_cross_entropy_loss(y_batch, y_pred)
                mse_loss = self.compute_mse_loss(y_batch,y_pred)
                acc = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_batch, axis=1))

                total_loss += loss * len(X_batch)
                total_ce_loss += ce_loss * len(X_batch)
                total_mse_loss += mse_loss * len(X_batch)
                total_acc += acc * len(X_batch)

                grads_W, grads_b = self.backward(X_batch, y_batch)

                self.t += 1
                if self.optimizer == "sgd":
                    sgd_update(self.weights, self.biases, grads_W, grads_b, self.learning_rate)
                elif self.optimizer == "momentum":
                    momentum_update(self.weights, self.biases, grads_W, grads_b, self.learning_rate, self.velocity_W, self.velocity_b)
                elif self.optimizer == "nesterov":
                    nesterov_update(self.weights, self.biases, grads_W, grads_b, self.learning_rate, self.velocity_W, self.velocity_b)
                elif self.optimizer == "rmsprop":
                    rmsprop_update(self.weights, self.biases, grads_W, grads_b, self.learning_rate, self.velocity_W, self.velocity_b, self.beta, self.epsilon)
                elif self.optimizer == "adam":
                    adam_update(self.weights, self.biases, grads_W, grads_b, self.learning_rate, self.velocity_W, self.velocity_b, self.moment2_W, self.moment2_b, self.beta1, self.beta2, self.epsilon, self.t)

            avg_loss = total_loss / num_samples
            avg_ce_loss = total_ce_loss / num_samples
            avg_mse_loss = total_mse_loss / num_samples
            avg_acc = total_acc / num_samples

            y_val_pred = self.forward(X_val)
            val_loss = self.compute_loss(y_val, y_val_pred)
            val_ce_loss = self.compute_cross_entropy_loss(y_val,y_val_pred)
            val_mse_loss = self.compute_mse_loss(y_val,y_val_pred)
            val_acc = np.mean(np.argmax(y_val_pred, axis=1) == np.argmax(y_val, axis=1))

            global best_model_data
            if val_acc > best_model_data["best_accuracy"]:
                best_model_data = {
                    "weights": [W.copy() for W in self.weights],
                    "biases": [b.copy() for b in self.biases],
                    "params": {
                        "layers": self.layers,
                        "learning_rate": self.learning_rate,
                        "activation": self.activation,
                        "optimizer": self.optimizer,
                        "weight_init": self.weight_init,
                        "weight_decay": self.weight_decay,
                        "beta": self.beta,
                        "beta1": self.beta1,
                        "beta2": self.beta2,
                        "epsilon": self.epsilon,
                    },
                    "best_accuracy": val_acc
                }
            
            wandb.log({
                "epoch": epoch + 1, 
                "loss": avg_loss, 
                "accuracy": avg_acc, 
                "val_loss": val_loss, 
                "val_accuracy": val_acc,
                "cross_entropy_loss": avg_ce_loss,
                "mse_loss": avg_mse_loss,
                "val_cross_entropy_loss": val_ce_loss,
                "val_mse_loss": val_mse_loss
            })
            
        y_val_pred = self.forward(X_val)
        y_val_pred_classes = np.argmax(y_val_pred, axis=1)
        y_val_true_classes = np.argmax(y_val, axis=1)
        
        cm = confusion_matrix(y_val_true_classes, y_val_pred_classes)
        
        class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                      'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
        
        wandb.log({
            "confusion_matrix": wandb.plot.confusion_matrix(
                probs=None,
                y_true=y_val_true_classes,
                preds=y_val_pred_classes,
                class_names=class_names
            )
        })

sweep_config = {
    'method': 'bayes',
    'metric': {'name': 'accuracy', 'goal': 'maximize'},
    'parameters': {
        'learning_rate': {'values': [1e-3,1e-4]},
        'batch_size': {'values': [16, 32, 64]},
        'epochs': {'values': [5, 10]},
        'hidden_layers': {'values': [3,4,5]},
        'hidden_size': {'values': [32, 64, 128]},
        'activation': {'values': ['relu', 'sigmoid','tanh']},
        'optimizer': {'values': ['sgd', 'momentum','nesterov','rmsprop','adam']},
        'weight_init': {'values': ['random', 'xavier']},
        'weight_decay': {"values": [0,0.0005,0.5]}
    },
    "run_cap":200
}


sweep_id = wandb.sweep(sweep_config, project="DLA1")

def train_with_wandb():
    wandb.init(project="DLA1", entity="da24m014") 
    config = wandb.config
    run_name = f"hl_{config.hidden_layers}_bs_{config.batch_size}_ac_{config.activation}"
    wandb.run.name = run_name
    model = NeuralNetwork([784] + [config.hidden_size] * config.hidden_layers + [10],
                          learning_rate=config.learning_rate,
                          activation=config.activation,
                          optimizer=config.optimizer,
                          weight_init=config.weight_init,
                          weight_decay=config.weight_decay,
                          )
    model.train(x_train, y_train, x_test, y_test, config.epochs, config.batch_size)
wandb.agent(sweep_id, function=train_with_wandb)

